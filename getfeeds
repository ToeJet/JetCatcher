# /bin/bash
# Simple RSS Downloader
# Ver  Date          Comments
# 1.0  02 May 2012   Initial write.
# 2.0  16 May 2012   Complete Rewrite.  Configuration too awkward
# 2.1  17 May 2012   Revised feed download to drop new file if no change from previous
# 2.2  22 May 2012   Revised download to only get files with extensions.
# 2.3  24 May 2012   Allowed ' or " to be used in feed.
# 2.4  03 Jun 2012   Reworked difference to remove duplicate downloads.
# 2.5  13 Jun 2012   Updated exclusion filters.  Changed to skip already downloaded.  Only log success.
# 2.6  20 Jun 2012   Updated exclusion filters.  Generate log of activity
# 2.7  20 Sep 2012   Updated comparison for feeds.
# 2.8  26 Nov 2012   Updated comparison for feeds.  Added additional exclusions
# 2.9  18 Apr 2014   Fixed Download file name to exclude {Space} and %
# 3.0  11 Jul 2014   Added Filters and comments to feed definition.
#                    Changed feed file read.   General Formatting cleanup
# 3.1  08 Jun 2015   Fixed issue with empty FEEDGREP
# 3.2  20 Feb 2017   Fixed issue with repeated download when URL and not filename changes. Changed to monthly log
# 3.3  03 Mar 2017   Added pdf to filter list.
#
# By James Toebes
# http://james.toebesacademy.com
# James@Toebesacademy.com
#
# Written from scratch with no reference.  Wanted a clean implementation.
# Use as you wish.  I accept no liability for it's use.
# Please email me before including it in another work, have questions,
# suggestions or comments.
#  
# Parses a file with same name as script with .feeds extension 
# downloads each feed.  Compares to previous download (if exist)
# then searches for links in difference.   Downloads each link.
#
# Installation/configuration is simple
# 1. Put script in folder.
# 2. create a text file same name as script with .feeds suffix.
#      1 line per rss feed.  no spaces or comments
# 3. Run script.
# two folders will be created in directory of script
#   feeds - Holds current, previous, and queued feed information
#   files - Downloaded files.
#      File names will be based on a cleaned up url. 
#      downloads from the same feed will have the same prefix.
# If first run gets you what you want, put it on cron.


GetFeed ()
{
	#Exit if nothing to do
	if [ -z "$1" ]
	then 
		return 0
	fi
	
    #Split arguments to feed and grep filter
    FEEDNAME=$1
    FEEDGREP=$2
 
    #Cleanup Feedname
    FEEDNAME=$(echo "${FEEDNAME}" | sed -e "s/[^a-zA-Z0-9:\/._=&\-]//g") #remove non valid characters
    FEEDGREP=$(echo "${FEEDGREP}" | sed -e "s/[^a-zA-Z0-9:\/._=&\-]//g") #remove non valid characters
    if [ -z "${FEEDGREP}" ] ; then FEEDGREP=. ; fi

    # Cleanup a feedname into a base name for files
    FEEDBASE=${FEEDNAME^^}
    FEEDBASE=${FEEDBASE#*\/\/} 		#strip http://, ftp://
    FEEDBASE=${FEEDBASE//OGG/}		#remove ogg
    FEEDBASE=${FEEDBASE//\./_}		#change . to _ for parsing
    FEEDBASE=${FEEDBASE//-/_}		#change - to _ for parsing
    FEEDBASE=${FEEDBASE//\//_}		#change / to _ for parsing
    FEEDBASE=${FEEDBASE//WWW_/}		#remove www.
    FEEDBASE=${FEEDBASE//_COM_/_}	#remove .com
    FEEDBASE=${FEEDBASE//_NET_/_}	#remove .net
    FEEDBASE=${FEEDBASE//_ORG_/_}	#remove .org
    FEEDBASE=${FEEDBASE//_PHP/}		#remove .php
    FEEDBASE=${FEEDBASE//_RSS/}		#remove .rss
    FEEDBASE=${FEEDBASE//_XML/}		#remove .xml
    FEEDBASE=${FEEDBASE//PODCASTS/}	#remove podcasts
    FEEDBASE=${FEEDBASE//PODCAST/}	#remove podcast
    FEEDBASE=${FEEDBASE//FEEDS/}	#remove feeds
    FEEDBASE=${FEEDBASE//FEED/}		#remove feed
    FEEDBASE=${FEEDBASE//OGG/}		#remove ogg
    FEEDBASE=${FEEDBASE//MP3/}		#remove mp3
    FEEDBASE=${FEEDBASE//HTML/}		#remove html
    FEEDBASE=${FEEDBASE//_}			#remove _, parsing done
    FEEDBASE=$(echo "${FEEDBASE}" | sed -e "s/[^a-zA-Z0-9._]//g") #remove non alphanum characters

    #downloads a feed.
    #adds changes to the download queue
    #echo ${FEEDNAME}
	
    # Names for download
    FEEDNEW=feeds/${FEEDBASE}.tmp
    FEEDPREV=feeds/${FEEDBASE}
    FEEDDOWN=feeds/${FEEDBASE}.dload

    # if current feed exist,  it must be a fail from a previous run. Delete it
    [ -e ${FEEDNEW} ] && rm ${FEEDNEW}

    # download feed. remove if any error
    wget  --quiet --no-check-certificate ${FEEDNAME} -O ${FEEDNEW}
    if [ $? != 0 ] || [ ! -e ${FEEDNEW} ]
    then
		echo "Error getting feed '${FEEDNAME}'">> $LOGNAME 
        [ -e ${FEEDNEW} ] && rm ${FEEDNEW}
    fi

    # if current feed exist,  it must be a fail from a previous run
    if [ -e ${FEEDNEW} ]
    then
        # check to see if files the same.  
        # If it is delete so last download trigger is file stamp
        # remove previous feed
        diff ${FEEDNEW} ${FEEDPREV} > /dev/null
        if [ $? -eq 0 ]
        then
            rm ${FEEDNEW}
        fi
    fi

    # look for changes - Add to DL Queue
    if [ -e ${FEEDNEW} ]
    then
        if [ -e ${FEEDPREV} ]
        then
            # Parse File, both new and previous (usually XML)
            # split to new lines on ' " < >, =  and {space} " '
            # remove any line without :// or a 3 to 4 character extension
            # sort to get unique names
            # apply grep filter as specified
            #
            # Compare results from each,
            # Only look at lines beginning with > (New lines)
            # change > to new line.
            # filter out all line not containing :// 
			
            for fname in $( diff \
                <(cat ${FEEDPREV} | tr "'" "\n" | tr "\"" "\n" | tr "\<" "\n" | tr "\>" "\n" | tr "=" "\n" | tr " " "\n" | tr " " "\n" | grep :// | grep '\..\{3,4\}$' | sed "s/^.*\///g" | sort -u | grep -i "${FEEDGREP}" )\
                <(cat ${FEEDNEW}  | tr "'" "\n" | tr "\"" "\n" | tr "\<" "\n" | tr "\>" "\n" | tr "=" "\n" | tr " " "\n" | tr " " "\n" | grep :// | grep '\..\{3,4\}$' | sed "s/^.*\///g" | sort -u | grep -i "${FEEDGREP}" )\
                | grep '^> ' | tr ">" "\n" | tr " " "\n" | sort -u | grep -v "^$")
            do 
                # get base name - check for file extension.
                FDOWN=${fname##*\/}
                FEXT=${FDOWN##*\.}
                case .${FEXT^^} in
                    # known to skip
                    . | .ATOM | .ASPX | .DTD | .COM | .CSS | .GIF | .HTM | .HTML | .JPEG | .JPG | .PHP | .NET | .ORG | .PDF | .PHP | .PNG | .RSS | .XML | .PDF | .SWF )
                        ;;

                    # known to download
                    .OGG | .MP3 | .MP4 | .M4A | .M4V )
                        echo $(date) ${FEEDNAME} NEW ${FEEDBASE}.${FDOWN} ${fname} >> $LOGNAME
                        echo ${fname} >>${FEEDDOWN}
                        ;;

                    #unknown extension.
                    *)
                        echo UNKNOWN EXTENSION -${FEXT}- ${fname}
                        echo UNKNOWN EXTENSION -${FEXT}- ${fname} >> $LOGNAME
                        ;;
                esac
            done

            # Done adding.
            # Remove History copy if it exist
            # rename previos to history
            if [ -e ${FEEDPREV}.1 ] 
            then
                rm ${FEEDPREV}.1
            fi
            mv ${FEEDPREV} ${FEEDPREV}.1
        fi

        # rename current to previous run
        mv ${FEEDNEW} ${FEEDPREV}
    fi
 
 
	
    # download files
    if [ -e ${FEEDDOWN} ]
    then
        # remove previous processing copy
        if [ -e ${FEEDDOWN}.tmp ]   
        then
            rm ${FEEDDOWN}.tmp
        fi
       
        # move file to processing copy
        mv ${FEEDDOWN} ${FEEDDOWN}.tmp
       
        # find new
        while read fname 
        do
			FURL=$(cat ${FEEDPREV}  | tr "'" "\n" | tr "\"" "\n" | tr "\<" "\n" | tr "\>" "\n" | tr "=" "\n" | tr " " "\n" | tr " " "\n" | grep :// | grep '\..\{3,4\}$' | sort -u | grep -i "${fname}" | head -1)
			#Don't download if it cannot be found in current feed.
			if [ ! -z "${FURL}" ]
			then 
				#FDOWN=${FURL##*\/}
				#FDOWN=${FDOWN//%/}
				FDOWN=$(echo "${fname}" | sed -e "s/[^a-zA-Z0-9._]//g") #remove non alphanum characters
				# skip if already downloaded
				if [ -e files/${FEEDBASE}.${FDOWN} ]
				then
					echo $(date) SKIPPING ${FEEDBASE}.${FDOWN} >> $LOGNAME 
				else
					# it has a file extension, download
					# echo Download: ${fname} 
					# echo       as: files/${FEEDBASE}.${FDOWN} 
					# remove file and add to retry on any fail
					wget --quiet --no-check-certificate  ${FURL} -O files/${FEEDBASE}.${FDOWN}
					if [ $? != 0 ]
					then
						[ -e files/${FEEDBASE}.${FDOWN} ] && rm files/${FEEDBASE}.${FDOWN} > /dev/null 
						#Download failed - Add to retry
						echo ${fname} >>${FEEDDOWN}
					else
						echo $(date) DOWNLOAD ${FEEDBASE}.${FDOWN} ${FURL} >> $LOGNAME 
					fi
 				fi 
            fi 
        done < ${FEEDDOWN}.tmp
       
        # cleanup
        rm ${FEEDDOWN}.tmp
    fi
}


#####
# Main Routine
#####

# working directory is folder of this script
cd "$(dirname "$(readlink -f "$0")")"

# Create folder to hold feeds and downloads
if [ ! -d feeds ]
then
    mkdir feeds
fi
if [ ! -d files ]
then
    mkdir files
fi

#Determine Log Name
#LOGNAME=$0.log
#Monthly Log Name
LOGNAME=$(basename $0).$(date +'%Y%m').log
if [ ! -f "${LOGNAME}" ]
then
	[ -e $(basename $0).log ] && rm $(basename $0).log
	touch ${LOGNAME}
	ln -s ${LOGNAME} $(basename $0).log
fi

#Process all feeds
export -f GetFeed
sed "s/#.*$//g" getfeeds.feeds | xargs -L 1 bash -c 'GetFeed $@' _

exit 0
